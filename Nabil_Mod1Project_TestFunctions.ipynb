{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT PACKAGES \n",
    "\n",
    "import requests\n",
    "import config\n",
    "import mysql.connector \n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION WILL REMOVE UNNEEDED KEYS\n",
    "#ENTER THE KEYS YOU WANT INTO THE KEYS1 LIST\n",
    "\n",
    "def dict_clean_businesses(df):\n",
    "    keys1 = ['id', 'vote_average', 'title', 'genre_ids', 'release_date', 'actor']\n",
    "    keys2 = list(df[0].keys())\n",
    "\n",
    "    for dictionary in df:\n",
    "        for key in keys2:\n",
    "            if key not in keys1:\n",
    "                del dictionary[key]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARSE GENRE TABLE VIA API\n",
    "\n",
    "def genre_id():\n",
    "    parameters = {'api_key': config.api_key} #these parameters go in to the URL\n",
    "    response = requests.get('https://api.themoviedb.org/3/genre/movie/list?language=en-US', params = parameters)\n",
    "    response.status_code #check this, should be 200\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FUNCTION TO OBTAIN MOVIE DICTIONARY PER PAGE\n",
    "\n",
    "def api_movies():\n",
    "    parameters = {'api_key': config.api_key, 'sort_by': 'vote_average.desc', 'vote_count.gte': 100, 'page': 1, 'with_original_language': 'en'}\n",
    "    #the 'page' key in the parameters will be a variable to be looped over\n",
    "    response = requests.get('https://api.themoviedb.org/3/discover/movie', params = parameters)\n",
    "    time.sleep(1)  #pauses code for a second to prevent too many parses\n",
    "    response.status_code #status code should be 200\n",
    "    \n",
    "    return response.json()['results']\n",
    "# api_movies()\n",
    "# dict_clean_businesses(webscrape())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOME WEBSCRAPE TESTING\n",
    "\n",
    "html_page = requests.get('https://www.themoviedb.org/movie/'+'182560'+'/cast')\n",
    "#182560 is a movie id 'variable' that will be iterated over\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "#standard syntax of webscraping\n",
    "\n",
    "#1) inspect page and find the actors hightlight\n",
    "#2) select the nearest tag w/ class = *div* container\n",
    "div = soup.find('ol', class_='people credits')\n",
    "all_divs = div.findAll('div', class_='info')\n",
    "#^find 'div' tag with class=info \n",
    "#and work from there using:\n",
    "#nextSibling, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nextSibling, previousSibling, children, parent, attrs, find(), findAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlize Theron',\n",
       " 'Nicholas Hoult',\n",
       " 'ChloÃ« Grace Moretz',\n",
       " 'Christina Hendricks',\n",
       " 'Tye Sheridan',\n",
       " 'Corey Stoll',\n",
       " 'Andrea Roth',\n",
       " 'Sterling Jerins',\n",
       " 'Shannon Kook',\n",
       " 'Drea de Matteo',\n",
       " 'Sean Bridgers',\n",
       " 'J. Larose',\n",
       " 'Jennifer Pierce Mathus',\n",
       " 'Denise Williamson',\n",
       " 'Natalie Precht',\n",
       " 'Madison McGuire',\n",
       " 'Addy Miller',\n",
       " 'Jeff Chase',\n",
       " 'Laura Cayouette',\n",
       " 'Richard Gunn',\n",
       " 'Glenn Morshower',\n",
       " 'Lori Z. Cordova',\n",
       " 'Michael Crabtree',\n",
       " 'John F. Daniel',\n",
       " 'Steve Shearer',\n",
       " 'Dan Hewitt Owens',\n",
       " 'Gillian Flynn',\n",
       " 'Tyler Forrest',\n",
       " 'Dora Madison']"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUNCTION TO GET 20 ACTORS FOR THE FIRST PAGE, THIS IS PRE-PAGINATION\n",
    "\n",
    "def movie_actors():\n",
    "    \n",
    "    html_page = requests.get('https://www.themoviedb.org/movie/'+'182560'+'/cast')\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    div = soup.find('ol', class_='people credits')\n",
    "    all_divs = div.findAll('div', class_='info')\n",
    "    \n",
    "    actors = []\n",
    "    for tag in all_divs:\n",
    "        actors.append(tag.text.split('\\n')[1])\n",
    "    return actors\n",
    "movie_actors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6885'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE: we are aiming on having 1000 movies and grabbing the actors from the movies as a table\n",
    "\n",
    "#WEBSRAPING FOR ACTOR ID BELOW\n",
    "div = soup.find('ol', class_='people credits') \n",
    "all_divs = div.findAll('div', class_='info')\n",
    "all_divs[0].find('a').attrs['href'].split('/')[2].split('-')[0]\n",
    "# div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acting'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEPS TAKEN TO WEBSCRAPE\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = requests.get('https://www.themoviedb.org/person/119589')\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "\n",
    "#inspect Donald Glover\n",
    "#acting\n",
    "\n",
    "div = soup.find('section', class_='facts left_column')\n",
    "div.findAll('p')\n",
    "list(div.findAll('p')[0].children)[0].text\n",
    "list(div.findAll('p')[0].children)[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acting', 'Male', '27', '1983-09-25', 'Stone Mountain, Georgia, USA']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUNCTION I CREATING TO OBTAIN 1 ACTOR'S INFO, SCRAPED AND EVERYTHING, NO PAGINATION IN THIS\n",
    "\n",
    "def actor_info():\n",
    "    html_page = requests.get('https://www.themoviedb.org/person/' + '119589')\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    div = soup.find('section', class_='facts left_column')\n",
    "    traits = []\n",
    "    for i in range(len(div.findAll('p'))-4):\n",
    "        traits.extend(list(div.findAll('p')[i].children)[1].split())\n",
    "    traits[4:8] = [' '.join(traits[4:8])]\n",
    "    \n",
    "#     traits.extend(traits[4].split(','))\n",
    "#     print(traits)\n",
    "#     del traits[4:6]\n",
    "#     del traits[5]\n",
    "#the above code is if we only want STATES (i.e. Georgia, California) the code is unfinished\n",
    "    return traits\n",
    "\n",
    "actor_info()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acting'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WEBSCRAPE TO OBTAIN THE \"KNOWN FOR:\" PORTION OF THE ACTOR\n",
    "\n",
    "list(div.findAll('p')[0].children)[1].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "1062 (23000): Duplicate entry '11969' for key 'PRIMARY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-992d08a90ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mpopulate_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'movie_info'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-992d08a90ba5>\u001b[0m in \u001b[0;36mpopulate_db\u001b[0;34m(records, table_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovie_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINSERT_QUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mysql/connector/cursor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmd_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterfaceError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_have_next_result\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=W0212\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36mcmd_query\u001b[0;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mServerCmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_have_next_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_handle_result\u001b[0;34m(self, packet)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpacket\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# We have a text result set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIntegrityError\u001b[0m: 1062 (23000): Duplicate entry '11969' for key 'PRIMARY'"
     ]
    }
   ],
   "source": [
    "#THIS IS WARREN'S CODE HE SLACKED TO ME SO THAT I COULD PAGINATE\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "import datetime\n",
    "import config\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "                host = \"rami-flatiron.ci2poaotvf7v.us-east-2.rds.amazonaws.com\",\n",
    "                user = \"admin_rami\",\n",
    "                passwd = \"oolP753cD635\",\n",
    "                database = 'movie_insights'\n",
    "                )\n",
    "c = conn.cursor()\n",
    "for i in range(38,52):\n",
    "    params = {'api_key': config.api_key, 'sort_by':'vote_average.desc', 'vote_count.gte': 100,\n",
    "              'page': i, \n",
    "              'with_original_language': 'en'}\n",
    "    base_url = config.base_url\n",
    "    response = requests.get(base_url + '/discover/movie', params = params)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Break at {i}, status code: {response.status_code}')\n",
    "        break\n",
    "    \n",
    "    less_info = list(map(lambda x: {'id':x['id'], \n",
    "                                    'vote_average': x['vote_average'],\n",
    "                                    'title': x['title'],\n",
    "#                                   'genre_ids': x['genre_ids'],\n",
    "                                   },response.json()['results']))\n",
    "    \n",
    "\n",
    "    def scrape_info(movie_dict):\n",
    "        movie_id = movie_dict['id']\n",
    "        html_page = requests.get('https://www.themoviedb.org/movie/' + str(movie_id)) #Make a get request to retrieve the page\n",
    "        soup = BeautifulSoup(html_page.content, 'html.parser') #Pass the page contents to beautiful soup for parsing\n",
    "        releases = soup.find('ul', class_=\"releases\")\n",
    "        revenue = releases.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling\n",
    "        try:\n",
    "            movie_dict['revenue'] = float(str(revenue.text).replace(revenue.strong.text, \"\").replace('$','').replace(',','').strip())\n",
    "        except:\n",
    "            movie_dict['revenue'] = None\n",
    "        budget = releases.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling\n",
    "        try:\n",
    "            movie_dict['budget'] = float(str(budget.text).replace(budget.strong.text, \"\").replace('$','').replace(',','').strip())\n",
    "        except:\n",
    "            movie_dict['budget'] = None\n",
    "        release_date = releases.div.previous_sibling.previous_sibling.previous_sibling.replace('\\n','').strip()\n",
    "        movie_dict['release_date'] = datetime.datetime.strptime(release_date, \"%B %d, %Y\").date()\n",
    "#         movie_dict['language'] = list(releases.nextSibling.nextSibling.children)[1].strip()\n",
    "#         html_page = requests.get('https://www.themoviedb.org/movie/'+ str(movie_id) + '/cast')\n",
    "#         soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "#         div = soup.find('ol', class_='people credits')\n",
    "#         all_divs = div.findAll('div', class_='info')\n",
    "    \n",
    "#         actors = []\n",
    "#         for tag in all_divs:\n",
    "#             actors.append(tag.text.split('\\n')[1])\n",
    "#         movie_dict['actors'] = actors\n",
    "        return movie_dict\n",
    "\n",
    "    movie_info = list(map(scrape_info,less_info))\n",
    "    \n",
    "    def populate_db(records, table_name):\n",
    "#         INSERT_STRING = (\"INSERT INTO {table_name} \"\n",
    "#                    \"(id, vote_average, title, genre_ids, revenue, budget, release_date) \"\n",
    "#                    \"VALUES (%s, %s, %s, %s)\")\n",
    "        INSERT_STR = f'INSERT INTO {table_name} (id, vote_average, title, revenue, budget, release_date) ' \n",
    "        VALUE_STR = 'VALUES (%s, %s, %s, %s, %s, %s)'\n",
    "        INSERT_QUERY = INSERT_STR + VALUE_STR\n",
    "    \n",
    "        for rec in movie_info:\n",
    "            values = tuple(rec.values())\n",
    "            c.execute(INSERT_QUERY, values)\n",
    "        conn.commit()\n",
    "\n",
    "    populate_db(movie_info,'movie_info')\n",
    "c.close()\n",
    "conn.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
